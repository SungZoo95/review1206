{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pytorch 특강 리뷰"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- import torch \n",
    "- torch.__version__ : 지금 사용하고 있는 torch 버전 불러온다\n",
    "- torch.cuda.is_available() : GPU환경인지 물어본다 참이면 True "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.Tensor vs torch.tensor \n",
    "\n",
    "- torch.Tensor\n",
    "    - 클래스 \n",
    "    - int 로 입력하면 float로 변환 1,2 --> 1.,2.\n",
    "    - list, numpy 입력 시 입력받은 데이터를 복사하여 새롭게 torch.Tensor 만든 후에 사용함\n",
    "- torch.tensor\n",
    "    - 함수\n",
    "    - int 입력하면 그대로 나옴 \n",
    "\n",
    "- 대소문자의 구분 확실하게 하자!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 대문자 Tensor\n",
    "    - original : tensor([1.]) new : tensor([1.]) 에서 오리지널 데이터를 수정하면 \n",
    "    - original : tensor([2.]) new : tensor([2.])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 소문자 tensor\n",
    "    - original : tensor([1]) new : tensor([1]) 에서 오리지널 데이터를 수정하면 \n",
    "    - original : [2] new : tensor([1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- list로 텐서 생성하기\n",
    "    - torch.Tensor(list)\n",
    "- numpy array로 텐서 생성하기\n",
    "    - torch.Tensor(numpy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GPU를 사용하려면 device 정보를 텐서에 string 타입으로 전달해줘야 한다.\n",
    "    - cuda : GPU\n",
    "    - cpu : CPU\n",
    "- .to(device) 로 데이터 정보 전달해주어야 사용가능하다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 랜덤 텐서 생성하기"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.manual_seed : 동일한 결과를 만들도록 seed 고정 (numpy랑 동일)\n",
    "- torch.rand : (0,1) 사이의 랜덤 텐서 생성 \n",
    "- torch.randn : 평균 =0 표준편차 =1인 정규분포로부터 랜덤텐서 생성\n",
    "- torch.randint : (최저값, 최대값) 사이에서 랜덤정수 텐서 생성 \n",
    "- numpy 랑 torch붙여 텐서 생성한다는거 빼곤 사용법이 같음 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.zeros_like : 사용하던 텐서와 같은 크기로 0으로 채워진 텐서 생성 \n",
    "- torch.ones_like : 사용하면 텐서와 같은 크기로 1로 채워진 텐서 생성\n",
    "- x.size() = x.shape 같음 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 뷰(view)\n",
    "- 텐서의 크기를 알맞게 변화할 때 사용 변화한 차원 크기는 원래 텐서의 차원 크기의 총 곱이 되어야 함  \n",
    "- 무슨소리인가? 크기가 (2,3,4) 3차원 텐서를 (2,2,6)으로 변경하면\n",
    "- (2,3,4) -->view(2,2,6)  2*3*4 = 24  2*2*6 = 24 같아야함\n",
    "- (-1)을 사용하면 나머지 차원을 계산해줍니다. \n",
    "- 무슨소리인가 ? (2,3,4) -> (-1,1,6) == (4,1,6) 으로 알아서 변경"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### permute\n",
    "- 차원의 위치를 바꿀때 주로 사용한다. \n",
    "- 무슨소리인가? 크기가 (2,3,4) 를 바꾸고싶다  차례대로 인덱스 0 1 2 번\n",
    "- permute(2,1,0) == (4,3,2) , permute(1,0,2) == (3,2,4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transpose\n",
    "- permute의 특별한 케이스 두 개 차원을 교환하여 바꿀때 사용\n",
    "- 무슨소리인가? 크기가 (3,4,5) permute와 같이 인덱스로 생각 (012)\n",
    "- transpose(1,0) -> (4,3,5) transpose(2,0)  -> (5,4,3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### squeeze & unsqueeze\n",
    "- squeeze : 텐서의 크기가 1인 차원을 지워버린다 . 숫자를 특정하면 해당 차원의 크기가 1일경우 지우고 아니면 그만 둔다 .\n",
    "- unsqueeze : 해당하는 숫자 차원에 크기 1인 차원을 늘린다 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 무슨 소리인가 ? 크기가 (2,1,3,4,1)인 차원에 squeeze를 적용하면 -> (2,3,4)\n",
    "- unsqueeze(3) -> (2,1,3,1,4,1) 해당 인덱스 위치에 1차원을 추가합니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cat & stack\n",
    "- cat & stack : 두 텐서를 합치는 역할을 담당한다. 차이는 이어짐과 끊어짐 \n",
    "- cat : 이어짐 예) tensor([[0]])  tensor([[1]]) cat 하면 tensor([[0],[1]])\n",
    "- stack : 끊어짐 tensor([[[0]],  [[0]]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 내적곱 (텐서의 곱셈)\n",
    "- torch.dot(x,y)를 사용 \n",
    "- 어떻게 사용하나?  torch.Tensor([1, 2, 3]),torch.Tensor([4, 5, 6])를 dot\n",
    "- 값은 tensor(32.) 왜? 1x4 + 2x5 + 3x6 = 32"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 행렬의 곱셈\n",
    "- torch.mm(x,y)를 사용 \n",
    "- 어떻게 사용하나 ? torch.Tensor([[1, 4], [2, 5], [3, 6]])\n",
    "torch.Tensor([[7, 9] [8, 10]])\n",
    "- mm을 사용하면 ? ([[39., 49.],[54., 68.],[69., 87.]])\n",
    "- 어떻게? 1x7+4x8 1x9+4x10 2x7+5x8 2x9+5x10 3x7+6x8 3x9+6x10 이나옵니다"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### max & argmax / min & argmin\n",
    "- 텐서의 최댓값 : max\n",
    "- 텐서의 최댓값의 인덱스 위치 :argmax\n",
    "- 텐서의 최솟값 : min\n",
    "- 텐서의 최솟값의 인덱스 위치 : argmin "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 논리연산\n",
    "- 0과 같다 : x.eq(0)\n",
    "- 0보다 크거나 같다 : x.ge(0) Great than or Equal to 0  GE\n",
    "- 0보다 크다 : x.gt(0) Great Than 0 GT \n",
    "- 0보다 작거나 같아 : x.le(0) Less than or Equal to 0 LE\n",
    "- 0보다 작다 : x.lt(0) Less Than 0 LT "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding Part \n",
    "- 두 벡터 w와x, 스칼라 b가 있다 함수를 작성해보자\n",
    "1. 두 벡터 (x,w)의 내적 값에 b를 더해서 z값을 구한다 .\n",
    "    - z = x.dot(w)+b == z = torch.dot(x,w) +b "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- import torch.nn.function as F (함수 불러오기)\n",
    "- F.softmax(tensor, dim=0) 기본식 텐서와 차원을 넣어줘야함 \n",
    "- 결과는 확률 0~1사이로 반환 \n",
    "- 저 반환한 값은 더하면 무조건 1이 되어야함 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch autograd \n",
    "-  requires_grad = True 미분을 한다는 뜻 \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch.nn\n",
    "- pytorch에서 미리 만들어준 툴 \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.nn.Linear \n",
    "```python\n",
    "X = torch.Tensor([[1, 2],\n",
    "                  [3, 4]])\n",
    "\n",
    "linear = nn.Linear(2,5) # 리니어로 만들어줄 차원을 설정해주고 \n",
    "output = linear(X) # 만들어준 리니어에 X를 집어넣어준다\n",
    "print(output)\n",
    "output.shape\n",
    "------\n",
    "tensor([[ 1.5422, -0.5694, -0.8987,  1.6633,  0.1518],\n",
    "        [ 2.7798, -2.0285, -2.1604,  4.2249,  0.3220]],\n",
    "       grad_fn=<AddmmBackward0>) # 차원이 바뀜 \n",
    "torch.Size([2, 5])\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Module \n",
    "- 모듈을 만드는 과정에서 super코드는 꼭 넣어주어야한다\n",
    "- super().__init__()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Sequential \n",
    "- 모듈들을 하나로 묶어 순차적으로 실행시키고 싶을때 사용\n",
    "- 어떻게? nn.Sequential(Add(3),Add(2),Add(5)) 이렇게 사용하면 사실상\n",
    "- x+3+2+5 의 순차적인 계산이 적용되는 것이다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.ModuleList\n",
    "- 원하는것만 인덱싱으로 쓸 수 있는 모듈 리스트 \n",
    "- 어떻게?\n",
    "```python \n",
    "self.add_list = nn.ModuleList([Add(2), Add(3), Add(5)])\n",
    "x = self.add_list[1](x)\n",
    "x = self.add_list[0](x)\n",
    "x = self.add_list[2](x)\n",
    "```\n",
    "- 이렇게 사용하면 ((x+3)+2)+5의 연산을 처리 가능합니다"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.ModuleDict\n",
    "- 리스트에 이은 딕셔너리 형태 key 값만 가져오면 됩니다.\n",
    "- 어떻게? \n",
    "```python\n",
    "self.add_dict = nn.ModuleDict({ 'add2': Add(2),\n",
    "                                'add3': Add(3),\n",
    "                                'add5': Add(5)})\n",
    "x = self.add_dict['add3'](x)\n",
    "x = self.add_dict['add2'](x)\n",
    "x = self.add_dict['add5'](x)                                 \n",
    "```\n",
    "- 이렇게 key 값을 가져와 ((x+3)+2)+5의 코드를 만들수도 있다"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- python의 list기능을 사용하지 않는 이유는 subModule로 등록이 되지 않아 나중에 모델을 출력해도 값이 나오지 않는다 그래서 pytorch의 제공함수로 사용해야 한다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter \n",
    "- W,b 를 만들 수 있습니다\n",
    "```python\n",
    "self.W = Parameter(torch.ones((out_features, in_features)))\n",
    "self.b = Parameter(torch.ones(out_features))\n",
    "``` \n",
    "- 이렇게 우리가 제공하지 않은 w와 b값을 제공받을 수 있고 \n",
    "이걸 사용해야만 gradient를 계산하는 함수인 grad_fn이 생성됩니다"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 의견\n",
    "- Pytorch는 Parameter로 지정된 tensor의 경우에\n",
    "back propagation에서 gradient값을 계산하여 값을 업데이트\n",
    "해주고 모델을 저장할 때 값을 지정해준다\n",
    "\n",
    "- Parameter로 지정하지 않으면 tensor의 값은 업데이트 되지도 않고 모델을 저장할때도 무시된다\n",
    "\n",
    "- 같은 tensor지만 일종의 표식을 남겨두어 특별 취급해주는 것이다\n",
    "linear transformation에서 w,b는 매우 중요해서 parameter로 지정해 준것이다. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buffer \n",
    "- 모델 저장시 값 저장을 할 수 있는 함수이다.\n",
    "```python\n",
    "self.register_buffer('buffer', self.tensor, persistent=True)\n",
    "``` \n",
    "- buffer(버퍼의변수명, 텐서, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
